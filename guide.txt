To run this project, please follow these steps in your terminal:

1. pip install -r requirements.txt , in the respective environment
2. python app.py - to run the project

After running the last command, open your web browser and navigate to the local URL provided in the terminal (usually http://127.0.0.1:7860) to use the application. 







LAYMAN EXPLANATION OF PROJECT : 

Below is a “from-scratch” walkthrough that avoids buzz-words and uses plain language.  Think of it as the story you would tell to a curious relative who has never written a line of code.

--------------------------------------------------------------------
1.  The Big Idea – Why Bother Adding Colour?
--------------------------------------------------------------------
• Old family photographs, newspaper archives, medical X-rays, or security-camera footage often exist only in black-and-white (grayscale).   
• Manually tinting every pixel with a paintbrush program is slow and requires artistic skill.   
• We want a computer program that can look at a grey picture and guess believable colours all by itself.  That process is called “image colourisation.”

--------------------------------------------------------------------
2.  How a Computer Sees a Picture
--------------------------------------------------------------------
• **Grayscale** picture: each pixel stores just one number – how bright it is (0 = black, 255 = white).  
• **Colour** picture: each pixel stores three numbers that mix red, green and blue light (called **RGB**).  For example, bright red is (255, 0, 0).  
• If we only have brightness, we must somehow invent the missing red/green/blue values.  That is the hard part.

--------------------------------------------------------------------
3.  A Trick Photographers Use – The Lab Colour Model
--------------------------------------------------------------------
• Scientists created another way to write down colours called **Lab**:  
  – **L** = Lightness (same thing as the grey image)  
  – **a** = how green-vs-red a pixel is  
  – **b** = how blue-vs-yellow a pixel is  
• This separation is handy: we already possess **L** (the grey photo).  We only need the computer to invent **a** and **b**.  Once we have all three, we convert back to ordinary RGB and show the picture.

--------------------------------------------------------------------
4.  Teaching a Computer to Guess Colours – What Is “Deep Learning”?
--------------------------------------------------------------------
• Think of a **neural network** as a very large stack of calculators.  
  – The first layer looks for simple things: edges and dots.  
  – Deeper layers combine these into shapes: eyes, wheels, leaves.  
  – The final layer makes a decision: “leaf → probably green,” “sky → probably blue.”  
• The network *learns* by seeing millions of already-coloured photos.  Each time its guess is wrong, it nudges its internal knobs (called *weights*) until the guess improves.  After months of practice on big computers, it becomes good at colourising new pictures it has never seen.

--------------------------------------------------------------------
5.  Ready-Made Brains – Pre-trained Models
--------------------------------------------------------------------
• Training from scratch is slow and needs expensive hardware.  
• Luckily, research teams released two “ready-made brains” (models) that have already learned the job:  
  1. **ECCV 2016 Model** – one of the first successful colourisers.  
  2. **SIGGRAPH 2017 Model** – a polished upgrade that usually gives smoother, more realistic results.  
• You can download their knowledge (the weight files) and use them instantly—just like buying a seasoned chef’s recipe book instead of inventing one.

--------------------------------------------------------------------
6.  What *Our* Project Adds – The Complete System
--------------------------------------------------------------------
1. **User Interface**  
   – We built a small web page (using a library called *Gradio*) where anyone can drag-and-drop a photo.  
   – A slider labelled “Colour Strength” lets the user choose subtle pastel tones or vivid colour.

2. **Pre-processing** (behind the scenes)  
   – The program shrinks the image to 256 × 256 pixels because the two brains expect that size.  
   – It converts RGB → Lab and extracts the **L** channel.

3. **Running the Brains**  
   – The grey **L** channel goes into each model.  
   – Each model outputs its guesses for the missing **a** and **b** channels.

4. **Post-processing**  
   – We merge **L** + guessed **a** + guessed **b** to form a full-colour Lab image.  
   – Convert Lab → RGB so browsers can display it.  
   – Blend with the original grey image according to the slider position.

5. **Side-by-Side Comparison**  
   – The web page shows two results next to each other (ECCV16 vs. SIGGRAPH17).  
   – Users can instantly see which one looks better for their particular photo.

--------------------------------------------------------------------
7.  Where the Engineering Effort Went
--------------------------------------------------------------------
• **Data Pipeline:** writing code that safely converts image formats, handles odd sizes, and keeps numbers in the ranges the brains expect.  
• **Model Integration:** downloading the weight files, loading them into memory, switching them to “evaluation mode,” and batching computations so everything stays fast.  
• **User Experience:** designing the layout, adding the strength slider, and making sure errors (wrong file types, huge photos) are politely reported.  
• **Testing & Debugging:** running many sample pictures (landscapes, portraits, cartoons) to ensure both models behave and the colours are not wildly wrong.

--------------------------------------------------------------------
8.  Real-World Uses
--------------------------------------------------------------------
• Restoring historical archives and family albums.  
• Quickly previewing colour schemes for black-and-white security footage.  
• Helping artists or filmmakers add realistic base colours before fine-tuning by hand.  
• Educational demos that show the power (and limits) of artificial intelligence in vision tasks.

--------------------------------------------------------------------
9.  Common Questions You Might Get
--------------------------------------------------------------------
• *“Does it always pick the ‘true’ colours?”*  
  No.  The brain guesses plausible colours based on patterns it saw during training.  A red car might turn blue if the model learned that blue cars are common.

• *“Why resize to 256 × 256? Won’t that reduce quality?”*  
  The brains were taught on that size.  Resizing guarantees the shapes it recognises line up correctly.  After colourisation we can scale back up; the colour information scales smoothly.

• *“Could we train it on our own photos?”*  
  Yes.  If you gather a large set of colour pictures, you can fine-tune the weights so the model adopts your specific style (e.g., comic-book colours or medical imagery).

--------------------------------------------------------------------
10.  One-Sentence Summary
--------------------------------------------------------------------
“Drop in a black-and-white photo, and our web app—powered by two expert AI ‘artists’—paints it with lifelike colours in seconds, letting you compare their styles and adjust the vividness to your taste.”






PROPER EXPLANATION : 

We'll structure this like a presentation:
1.  **The Fundamental Problem:** What is image colorization and why is it hard?
2.  **The Core Concept:** The "Magic" of the Lab Color Space.
3.  **The Technology:** How Convolutional Neural Networks (CNNs) Learn to Color.
4.  **Our Project's Architecture:** A Step-by-Step Walkthrough of Your Code.
5.  **The Specific Models:** ECCV16 vs. SIGGRAPH17.
6.  **Your Contribution:** What You Actually Built (The Engineering Work).
7.  **Potential Questions & How to Answer Them.**

---

### 1. The Fundamental Problem: What is Image Colorization?

Image colorization is the process of adding color to a monochrome (grayscale) image. For a human, this is an intuitive process. We know that skies are typically blue, grass is green, and a stop sign is red.

For a computer, this is an incredibly difficult problem. A grayscale image only contains **intensity** or **brightness** information for each pixel (from black to white). It has no color information. The computer must *infer* the correct color based on the object and its context.

This is an **ill-posed problem**, meaning there's no single, correct answer. An apple could be red, green, or yellow. A car could be any color. The goal of an AI model is to predict a **plausible** and **perceptually realistic** color for every part of the image.

### 2. The Core Concept: The "Magic" of the Lab Color Space

This is the most important concept to understand. Your project doesn't work in the standard **RGB** (Red, Green, Blue) color space. It uses the **CIE Lab** color space.

*   **RGB:** Describes a color by mixing amounts of Red, Green, and Blue. The problem is, brightness and color information are tangled together. You can't change the brightness without changing the R, G, and B values.
*   **Lab:** This is designed to be more like how humans perceive color. It separates brightness from the color itself.
    *   **L Channel (Lightness):** This is the grayscale representation of the image. It goes from 0 (pure black) to 100 (pure white). **This is your input.**
    *   **a Channel:** This represents the spectrum from green to red.
    *   **b Channel:** This represents the spectrum from blue to yellow.

**The Key Insight:** We can take a black and white photo (which is just the **L** channel), feed it to our AI, and train the AI to predict the **a** and **b** channels. Then, we just stack the original **L** channel with the predicted **a** and **b** channels to get a full-color image!

This transforms the problem from "guess the RGB values" to "given this brightness and these shapes (L), what are the most probable color values (a and b)?"

### 3. The Technology: How CNNs Learn to Color

Your project uses **Convolutional Neural Networks (CNNs)**. These are the standard for any AI task involving images.

Think of it like a human artist who has studied millions of photographs.
*   The artist learns that a certain texture and shape (like blades of grass) is usually green.
*   They learn that a large, smooth area at the top of an outdoor picture is usually blue (a sky).
*   They learn that the specific shape of a human face has a certain range of skin tones.

A CNN does the same thing, but with math. It scans the input image (the **L** channel) with millions of tiny filters (kernels) to learn patterns: edges, textures, shapes, and gradients. Through its many layers, it builds a complex understanding of the objects in the image.

The `eccv16` and `siggraph17` models are just sophisticated CNN architectures that have already been "taught" these patterns by being trained on a massive dataset of color images (like ImageNet). They were shown the **L** channel and forced to guess the **a** and **b** channels, and they got "smarter" with every correct and incorrect guess.

### 4. Our Project's Architecture: A Step-by-Step Walkthrough

This is what *your* code (`app.py`) does. It's the pipeline you engineered.

1.  **Input:** The user uploads an image via the Gradio web interface.
2.  **Pre-processing:**
    *   The image is resized to 256x256 pixels. **Why?** Because the pre-trained models were trained on images of this size. They expect this specific input dimension.
    *   The resized RGB image is converted to the **Lab color space**.
    *   The **L** channel is separated from the 'a' and 'b' channels.
    *   The **L** channel is converted into a PyTorch Tensor, which is the data format the models understand.
3.  **Inference (Prediction):**
    *   The **L** channel tensor is fed into the pre-trained `colorizer_eccv16` model. The model does its magic and outputs a prediction for the **a** and **b** channels.
    *   The **L** channel tensor is also fed into the pre-trained `colorizer_siggraph17` model, which makes its own, separate prediction for the **a** and **b** channels.
4.  **Post-processing:**
    *   For each model's output, we take the original **L** channel and combine it with the predicted **a** and **b** channels. Now we have two full Lab images.
    *   These Lab images are converted back to the standard **RGB** color space. **Why?** Because computer monitors and web browsers display images in RGB.
5.  **Blending & Output:**
    *   The "Colorization Strength" slider is used to blend the newly colorized image with the original grayscale image. This is a simple linear interpolation that gives the user artistic control.
    *   The two final RGB images are sent back to the Gradio interface to be displayed side-by-side.

### 5. The Specific Models: ECCV16 vs. SIGGRAPH17

You are using two well-known models. Knowing the difference is a sign of deeper understanding.

*   **ECCV16 ("Colorful Image Colorization"):** This was a pioneering model. Its key idea was to treat colorization as a *classification* problem. For each pixel, instead of just guessing one color, it predicted a *probability distribution* over a range of possible colors. This often results in more vibrant and saturated colors, but can sometimes look a bit unnatural.
*   **SIGGRAPH17 (An improvement on the same idea):** This model refined the approach. It's also a CNN, but with a more complex architecture. It often produces more realistic and spatially coherent results than the ECCV16 model. It's generally considered an improvement.

Your project's side-by-side comparison allows a user to directly see the trade-offs between these two famous approaches.

### 6. Your Contribution: What You Actually Built

This is the answer to "So what did *you* do?".

*You were the AI Systems Engineer.* You didn't invent the theory, you **built the system**.
1.  **Integrated two separate, complex AI models** into one application.
2.  **Engineered the entire data pipeline:** from user upload -> pre-processing -> model inference -> post-processing -> final display.
3.  **Built an interactive user interface** using Gradio, making the technology accessible to non-technical users.
4.  **Created a platform for comparative analysis**, allowing for a direct, visual comparison of two state-of-the-art models.
5.  **Added user-centric features** like the "Colorization Strength" slider, showing that you thought about the end-user experience.

---

### 7. Potential Questions & How to Answer Them

*   **Q: Why use the Lab color space instead of just working with RGB?**
    *   **A:** "Because the Lab color space separates brightness (L) from color (a, b). This lets us frame the problem perfectly: we can use the original grayscale image, which is the L channel, as our direct input and train the model to predict only the missing color information. In RGB, brightness and color are entangled, making it a much harder problem for the model to learn."

*   **Q: You didn't train the models. So, what was the main challenge?**
    *   **A:** "The main challenge was in systems integration and building a robust data pipeline. The pre-trained models are powerful but have very strict requirements for their input data—the image must be a specific size, in a specific color space, and formatted as a tensor. My work was to engineer the software that correctly prepares the data, manages the models, processes their output, and presents it to the user in an intuitive way. It's the difference between having an engine and building a fully functional car around it."

*   **Q: What are the limitations of this approach?**
    *   **A:** "The biggest limitation is context dependency. The models color based on what they've seen in their training data. If they see a picture of a historical figure in a military uniform, they might color it incorrectly if they've never seen that specific uniform before. They also struggle with purely abstract or fantastical images. The quality of the colorization is entirely dependent on the knowledge captured in the pre-trained weights."

*   **Q: How would you improve this project?**
    *   **A:** "There are a few ways. First, I would explore adding a 'fine-tuning' feature, where a user could provide a few color 'hints' or scribbles on the image, and the model would use those hints to guide the colorization. Second, I would investigate more recent, even more powerful models, like those based on the Transformer architecture, to see if they produce better results. Finally, I would add a more robust user feedback system to flag cases where the models make mistakes, which could be used to improve them in the future."

*   **Q: What is that 'Colorization Strength' slider doing mathematically?**
    *   **A:** "It's performing a linear interpolation between the original grayscale image (represented in RGB) and the model's colorized RGB output. The formula is `Final Pixel = (1 - strength) * Original Pixel + strength * Colorized Pixel`. When strength is 1.0, you see 100% of the colorized image. When it's 0.5, you're seeing a 50/50 blend."